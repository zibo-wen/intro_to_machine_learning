# -*- coding: utf-8 -*-
"""a1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12VT3wbohdng7Qohr5Sr_Nf00F3JDgPZ0
"""

%tensorflow_version 1.x

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt


EPSILON_0 = 1e-15
LOSS_TYPE = ''

# Reads the data and divide it into training, validation and testing sets
def loadData():
    with np.load('notMNIST.npz') as data :
        Data, Target = data ['images'], data['labels']
        posClass = 2
        negClass = 9
        dataIndx = (Target==posClass) + (Target==negClass)
        Data = Data[dataIndx]/255.
        Target = Target[dataIndx].reshape(-1, 1)
        Target[Target==posClass] = 1
        Target[Target==negClass] = 0
        np.random.seed(421)
        randIndx = np.arange(len(Data))
        np.random.shuffle(randIndx)
        Data, Target = Data[randIndx], Target[randIndx]
        trainData, trainTarget = Data[:3500], Target[:3500]
        validData, validTarget = Data[3500:3600], Target[3500:3600]
        testData, testTarget = Data[3600:], Target[3600:]
    return trainData, validData, testData, trainTarget, validTarget, testTarget

# Calculate Mean Square Error
def MSE(W, b, x, y, reg):
    
    # Transform the 2D image data into 1D array
    length = len(x[0])*len(x[0][0])
    x_matrix = np.reshape(x,(len(x),length))
    
    mse = np.sum(np.square(np.matmul(x_matrix,W)+b-y))/len(x) + (reg/2)*np.matmul(np.transpose(W),W)
    return mse[0][0];

# Calculate gradient of MSE with respect to bias and weights
def gradMSE(W, b, x, y, reg):
    length = len(x[0])*len(x[0][0]) #784
    x_matrix = np.reshape(x,(len(x),length))
    error = np.matmul(x_matrix,W) + b - y
    grad_w = np.matmul(np.transpose(x_matrix), error) * 2/len(x) + reg * W
    grad_b = np.sum(error)*2/len(x)
    return grad_w, grad_b;

# Calculates the (number of correct estimates)/(number of true labels)
def accuracy(weight, bias, data, trueLabel):
    length = len(data[0]) * len(data[0][0])
    if LOSS_TYPE == 'MSE':
        result = np.multiply(np.matmul(np.reshape(data, (len(data), length)), weight) + bias - 0.5, trueLabel - 0.5)
    elif LOSS_TYPE == 'CE':
        data = data.reshape((data.shape[0], data.shape[1] * data.shape[2]))
        result = np.multiply(1 / (1 + np.exp(-(np.dot(data, weight) + bias))) - 0.5, trueLabel - 0.5)
    numCorrect = np.sum(result > 0)
    return numCorrect / len(trueLabel);

# Optimum weights using the closed form equation for the derivative of the MSE
def normalEquation(data, reg, trueLabel):
    length = len(data[0])*len(data[0][0])
    data_matrix = np.reshape(data,(3500,length))
    pseudoInvert = np.linalg.inv(np.matmul(np.transpose(data_matrix),data_matrix) + reg * np.identity(length));
    weight = np.matmul(np.matmul(pseudoInvert, np.transpose(data_matrix)),trueLabel-0.5)
    return weight;


'''
**********************************************************

    Part 2 Functions:
        crossEntropyLoss
        gradCE
        grad_descent
                    
**********************************************************

'''
# Calculate cross entropy loss for logistic regression
def crossEntropyLoss(W, b, x, y, reg):
    N = len(y)
    x = x.reshape((x.shape[0], x.shape[1] * x.shape[2]))
    sigma = 1 / (1 + np.exp(- (np.dot(x, W) + b)))
    for i in range(len(sigma)):
        if sigma[i] == 0: 
            sigma[i] = EPSILON_0
        elif sigma[i] == 1:             
            sigma[i] = 1-EPSILON_0

    loss_D = - (y * (np.log(sigma)) + (1-y) * np.log(1 - sigma))
    loss_D = 1/N * loss_D.sum()
    loss_W = (reg / 2) * (np.linalg.norm(W) ** 2)
    return loss_D + loss_W

# Calculate gradient of CE
def gradCE(W, b, x, y, reg):
    N = len(y)
    x = x.reshape((x.shape[0], x.shape[1] * x.shape[2]))
    sigma = 1 / (1 + np.exp(- x @ W - b))
    grad_B = 1/N * (sigma - y).sum()
    grad_W = 1/N * (x.T @ (sigma - y)) + reg * W
    return grad_W, grad_B

# Gradient descent model training for both MSE and CE
def grad_descent(W, b, x, y, alpha, epochs, reg, error_tol,
                 validData, testData, validTarget, testTarget, lossType="MSE"):
    trainingError = []
    validationError = []
    testError = []
    trainingAccuracy = []
    validationAccuracy = []
    testAccuracy = []

    if lossType == 'MSE':
        for i in range(epochs):
            trainingError.append(MSE(W, b, x, y, reg))
            validationError.append(MSE(W, b, validData, validTarget, reg))
            testError.append(MSE(W, b, testData, testTarget, reg))
            trainingAccuracy.append(accuracy(W, b, x, y))
            validationAccuracy.append(accuracy(W, b, validData, validTarget))
            testAccuracy.append(accuracy(W, b, testData, testTarget))

            grad_w, grad_b = gradMSE(W, b, x, y, reg)
            oldW = W
            W = W - alpha * grad_w
            b = b - alpha * grad_b
            if np.linalg.norm(oldW-W) < error_tol :
                break
            
    elif lossType == "CE":
        for i in range(epochs):
            trainingError.append(crossEntropyLoss(W, b, x, y, reg))
            validationError.append(crossEntropyLoss(W, b, validData, validTarget, reg))
            testError.append(crossEntropyLoss(W, b, testData, testTarget, reg))
            trainingAccuracy.append(accuracy(W, b, x, y))
            validationAccuracy.append(accuracy(W, b, validData, validTarget))
            testAccuracy.append(accuracy(W, b, testData, testTarget))

            
            grad_w, grad_b = gradCE(W, b, x, y, reg)
            oldW = W
            W = W - alpha * grad_w
            b = b - alpha * grad_b
            if np.linalg.norm(oldW-W) < error_tol :
                break      

    # Print Error
    plt.figure(1, figsize=(5, 5))
    trainingErrorLine, = plt.plot(trainingError, label='trainingError')
    validationErrorLine, = plt.plot(validationError, label='validationError')
    testErrorLine, = plt.plot(testError, label='testError')
    plt.ylabel('Error')
    plt.xlabel('Epoch')
    plt.title('Errors in Epoches')
    plt.legend([trainingErrorLine, validationErrorLine, testErrorLine], ['trainingError', 'validationError', 'testError'])

    # Print Accuracy
    plt.figure(2, figsize=(5, 5))
    trainingAccuracyLine, = plt.plot(trainingAccuracy, label='trainingAccuracy')
    validationAccuracyLine, = plt.plot(validationAccuracy, label='validationAccuracy')
    testAccuracyLine, = plt.plot(testAccuracy, label='testAccuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.title('Accuracy in Epoches')
    plt.legend([trainingAccuracyLine, validationAccuracyLine, testAccuracyLine], ['trainingAccuracy', 'validationAccuracy', 'testAccuracy'])

    plt.show()
    
    return W,b

# Initialize the Tensorow computational graph
def buildGraph(lossType, alpha, beta1, beta2, epsilon, batchSize):
    #Initialize weight and bias tensors
    weight = tf.Variable(tf.truncated_normal(shape=(784, 1), stddev=0.5, dtype=tf.float32))
    bias = tf.Variable(0.5)

    data = tf.placeholder(tf.float32, shape=(batchSize, 784), name="data")
    labels = tf.placeholder(tf.float32, shape=(batchSize, 1), name="labels")
    reg = tf.placeholder(tf.float32)
    
    theLoss = 0;3
    predictions = 0;
    
    tf.set_random_seed(421)

    if lossType == "MSE":
        predictions = tf.matmul(data, weight)+bias
        theLoss = tf.losses.mean_squared_error(labels=labels, predictions=predictions) + reg/2*tf.reduce_sum(tf.math.square(weight))
    
    elif lossType == "CE":
        predictions = tf.sigmoid(tf.matmul(data, weight) + tf.convert_to_tensor(bias))
        theLoss = tf.losses.sigmoid_cross_entropy(labels, predictions) + reg/2*tf.reduce_sum(tf.math.square(weight))
    
    optimizer = tf.train.AdamOptimizer(learning_rate=alpha, beta1=beta1, beta2=beta2, epsilon=epsilon).minimize(theLoss)
    return weight,bias,reg,predictions,data,labels,theLoss,optimizer

# Apply SGD algorithm to optimize the training process using Adam
def SGD(batchSize, epoch, lossType, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):
    global LOSS_TYPE
    LOSS_TYPE = lossType

    trainData, validData, testData, trainTarget, validTarget, testTarget = loadData();
    W,b,reg,predictions,data,labels,loss,optimizer = buildGraph(lossType, alpha, beta1, beta2, epsilon, batchSize);

    init_op = tf.global_variables_initializer()

    batchAmount = int(len(trainData)/batchSize)
    length = len(trainData[0])*len(trainData[0][0]) #784
    trainDataReshaped = np.reshape(trainData,(3500,length))

    trainingError = []
    validationError = []
    testError = []
    trainingErrorCE = []
    validationErrorCE = []
    testErrorCE = []
    trainingAccuracy = []
    validationAccuracy = []
    testAccuracy = []
    reglambda = 0

    with tf.Session() as sess:
        sess.run(init_op)
        for i in range(epoch):
            for j in range(batchAmount):
                dataBatch = trainDataReshaped[j*batchSize:(j+1)*batchSize]
                labelBatch = trainTarget[j*batchSize:(j+1)*batchSize]
                feed_dict={labels: labelBatch,data: dataBatch,reg: reglambda}
                sess.run(optimizer, feed_dict=feed_dict);
                
            if lossType == 'CE':
                trainingError.append(crossEntropyLoss(W.eval(), b.eval(), trainData, trainTarget, reglambda))
                validationError.append(crossEntropyLoss(W.eval(), b.eval(), validData, validTarget, reglambda))
                testError.append(crossEntropyLoss(W.eval(), b.eval(), testData, testTarget, reglambda))
                
            elif lossType == 'MSE':
                trainingError.append(MSE(W.eval(), b.eval(), trainData, trainTarget, reglambda))
                validationError.append(MSE(W.eval(), b.eval(), validData, validTarget, reglambda))
                testError.append(MSE(W.eval(), b.eval(), testData, testTarget, reglambda))
            trainingAccuracy.append(accuracy(W.eval(), b.eval(), trainData, trainTarget))
            validationAccuracy.append(accuracy(W.eval(), b.eval(), validData, validTarget))
            testAccuracy.append(accuracy(W.eval(), b.eval(), testData, testTarget))

        # Print error
        plt.figure(1, figsize=(5, 5))
        trainingErrorLine, = plt.plot(trainingError, label='trainingError')
        validationErrorLine, = plt.plot(validationError, label='validationError')
        testErrorLine, = plt.plot(testError, label='testError')
        plt.ylabel('Error')
        plt.xlabel('Epoch')
        plt.title('Errors in Epoches')
        plt.legend([trainingErrorLine, validationErrorLine, testErrorLine], ['trainingError', 'validationError', 'testError'])

        # Print accuracy
        plt.figure(2, figsize=(5, 5))
        trainingAccuracyLine, = plt.plot(trainingAccuracy, label='trainingAccuracy')
        validationAccuracyLine, = plt.plot(validationAccuracy, label='validationAccuracy')
        testAccuracyLine, = plt.plot(testAccuracy, label='testAccuracy')
        plt.ylabel('Accuracy')
        plt.xlabel('Epoch')
        plt.title('Accuracy in Epoches')
        plt.legend([trainingAccuracyLine, validationAccuracyLine, testAccuracyLine], ['trainingAccuracy', 'validationAccuracy', 'testAccuracy'])
        
        plt.show()
            
        print('bias',b.eval())
        print('trainDataAccuracy',accuracy(W.eval(),b.eval(),trainData,trainTarget))
        print('validDataAccuracy',accuracy(W.eval(),b.eval(),validData,validTarget))
        print('testDataAccuracy',accuracy(W.eval(),b.eval(),testData,testTarget))
        if lossType == 'MSE':
            print('trainDataError',MSE(W.eval(), b.eval(), trainData, trainTarget, 0))
            print('validDataError',MSE(W.eval(), b.eval(), validData, validTarget, 0))
            print('testDataError',MSE(W.eval(), b.eval(), testData, testTarget, 0))
        elif lossType == 'CE':
            print('trainDataError',crossEntropyLoss(W.eval(), b.eval(), trainData, trainTarget, 0))
            print('validDataError',crossEntropyLoss(W.eval(), b.eval(), validData, validTarget, 0))
            print('testDataError',crossEntropyLoss(W.eval(), b.eval(), testData, testTarget, 0))


if __name__ == '__main__':
    # For Section 2.1.2 CE Logistic Regression Model - Learning
    LOSS_TYPE = 'CE'
    trainData, validData, testData, trainTarget, validTarget, testTarget = loadData();
    W = np.empty((784, 1))
    W.fill(0.1)
    b = 0.5
    reg = 0.001
    W, b = grad_descent(W, b, trainData, trainTarget, 0.005, 5000, reg, 
                        1E-7, validData, testData, validTarget, testTarget, LOSS_TYPE);
    
    # For Section 3.1.5 Batch Size Investigation
    LOSS_TYPE = 'CE'
    batchSizeList = [500, 100, 700, 1750]
    for batchSize in batchSizeList:
        print('\n\n(a) ' + LOSS_TYPE + 
              ' SGD with batchSize=' + str(batchSize))
        SGD(batchSize=batchSize, epoch=700, lossType = LOSS_TYPE) 
    

    
