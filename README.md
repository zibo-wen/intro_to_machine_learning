# Introduction to Machine Learning

This repo contains assignments for the course Introduction to Machine Learning in the University of Toronto, completed by Zibo Wen and Yunyi Zhu. 

PLEASE USE FOR REFERENCE ONLY. DO NOT COPY.

## [Assignment 1  - Linear and Logistic Regression](/A1%20-%20Linear%20and%20Logistic%20Regression)

The purpose of this lab is to investigate the classification performance of linear and logistic regression. We first implemented a linear regression classifier and a logistic regression classifier in NumPy, and then applied a batch Gradient Descent optimization algorithm to both of the learning models. The learning results from both models are investigated, evaluated and compared against ADAM using Stochastic Gradient Descent which is implemented by TensorFlow. The learning objective is to recognize the letter "C" (the positive class) and letter "J" (the negative class) from a dataset containing images of these two letters.

<p align="center">
  <img src="/img/A1_Dataset.png" alt=" A1 Dataset " >
</p>

## [Assignment 2 - Neural Networks](/A2%20-%20Neural%20Networks)

The purpose of this assignment is to investigate the classification performance of neural networks. In the first part, a neural network model using NumPy is implemented, and in the second part another model with the same function is built in TensorFlow. 

The neural network models developed in both parts will be used for letter recognition. The dataset that we will use in this assignment is a permuted version of notMNIST1 used in the Linear and Logistic Regression study, which contains 28-by-28 images of 10 letters (A to J) in different fonts. This dataset has 18720 instances, which are divided into different sets for training, validation and testing and fed into the models.

<p align="center">
  <img src="/img/A2_Dataset.png" alt=" A2 Dataset " >
</p>

## [Assignment 3 â€“ Unsupervised Learning and Probabilistic Models](/A3%20-%20Unsupervised%20Learning%20and%20Probabilistic%20Models/)

Two types of unsupervised learning models are implemented in this assignment, where the task is to summarize the simulated datasets into different clusters. In the first part, a K-means learning model is developed, by alternating between assigning data points in clusters and updating the cluster center; in the second part, a probabilistic version of K-means clustering, the Mixtures of Gaussians (MoG) is implemented. The effect of choosing different numbers of cluster centers in each model is also investigated.

Here is an example clustering result generated by the MoG model:
<p align="center">
  <img src="/img/A3_MoG_Example.png" alt="A3 MoG" width="300">
</p>

